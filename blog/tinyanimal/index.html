<!doctype html><html lang=en-US><head><meta charset=utf-8><meta content="ie=edge" http-equiv=x-ua-compatible><meta content="width=device-width,initial-scale=1,shrink-to-fit=no" name=viewport><style>/* latin */
    @font-face {
      font-family: 'Rubik';
      font-style: normal;
      font-weight: 400;
      src: url(/fonts/rubik.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }

    * {
      font-family: "Rubik";
    }</style><link href=/css/line-awesome.min.css rel=stylesheet><link href=/asciinema-player.css rel=stylesheet><link as=font crossorigin=anonymous href=https://joinbase.io/fonts/vendor/jost/jost-v4-latin-regular.woff2 rel=preload type=font/woff2><link as=font crossorigin=anonymous href=https://joinbase.io/fonts/vendor/jost/jost-v4-latin-700.woff2 rel=preload type=font/woff2><link href=https://joinbase.io/main.css rel=stylesheet><meta content="index, follow" name=robots><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=googlebot><meta content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" name=bingbot><title>TinyAnimal: Animal Recognition Practices on Grove Vision AI | JoinBase</title><meta content="JoinBase, a single-binary AIoT-first data-service platform." name=description><link href=https://joinbase.io/blog/tinyanimal/ rel=canonical><meta content="TinyAnimal: Animal Recognition Practices on Grove Vision AI" property=og:title><meta content="JoinBase, a single-binary AIoT-first data-service platform." property=og:description><meta content=article property=og:type><meta content=https://joinbase.io/blog/tinyanimal/ property=og:url><meta content=tb-logo.png property=og:image><meta content=2023-02-21T12:00:00+00:00 property=og:updated_time><meta content="TinyAnimal: Animal Recognition Practices on Grove Vision AI" property=og:site_name><meta content=en_US property=og:locale><script type=application/ld+json>
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/blog/tinyanimal/"
      },
      "headline": "TinyAnimal: Animal Recognition Practices on Grove Vision AI",
      "image": ,
      "datePublished": "2023-02-20T12:00:00+00:00",
      "dateModified": "2023-02-21T12:00:00+00:00",
      "author": {
        "@type": "Organization",
        "name": "TinyAnimal: Animal Recognition Practices on Grove Vision AI"
      },
      "publisher": {
        "@type": "Organization",
        "name": "TinyAnimal: Animal Recognition Practices on Grove Vision AI",
        
        "logo": {
          "@type": "ImageObject",
          "url": "/tb-logo.png"
        }
        
      },
      "description": "JoinBase, a single-binary AIoT-first data-service platform."
    }
    </script><script type=application/ld+json>
  {
    "@context": "http://schema.org",
    "@type": "BreadcrumbList",
    
      
      
        
        
        
        
        
        
        
        
          {
            "@type": "ListItem",
            "position":  1 ,
            "name": "Home",
            "item": "https://joinbase.io/"
          },
          
          
          {
            "@type": "ListItem",
            "position":  2 ,
            "name": "Blog",
            "item": "https://joinbase.io/blog/"
          },
        
      
        
        
        
        
        
        
        
        
          
          
          {
            "@type": "ListItem",
            "position":  3 ,
            "name": "Tinyanimal",
            "item": "https://joinbase.io/blog/tinyanimal/"
          },
        
      
    
  }
</script><meta content=#000 name=theme-color><link href=/joinbase_logo_180.png rel=apple-touch-icon sizes=180x180><link href=/joinbase_logo_32.png rel=icon sizes=32x32 type=image/png><link href=/joinbase_logo_16.png rel=icon sizes=16x16 type=image/png><body class="blog single"><div class="header-bar fixed-top"></div><header class="navbar fixed-top navbar-expand-md navbar-light"><div class=container><input class="menu-btn order-0" id=menu-btn type=checkbox><label class="menu-icon d-md-none" for=menu-btn><span class=navicon></span></label><a class="navbar-brand order-1 order-md-0 me-auto" href=https://joinbase.io>JoinBase</a><button aria-label="Toggle mode" class="btn btn-link order-2 order-md-4" id=mode type=button><span class=toggle-dark><svg class="feather feather-moon" viewbox="0 0 24 24" fill=none height=20 stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 width=20 xmlns=http://www.w3.org/2000/svg><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span> <span class=toggle-light><svg class="feather feather-sun" viewbox="0 0 24 24" fill=none height=20 stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 width=20 xmlns=http://www.w3.org/2000/svg><circle cx=12 cy=12 r=5></circle><line x1=12 x2=12 y1=1 y2=3></line><line x1=12 x2=12 y1=21 y2=23></line><line x1=4.22 x2=5.64 y1=4.22 y2=5.64></line><line x1=18.36 x2=19.78 y1=18.36 y2=19.78></line><line x1=1 x2=3 y1=12 y2=12></line><line x1=21 x2=23 y1=12 y2=12></line><line x1=4.22 x2=5.64 y1=19.78 y2=18.36></line><line x1=18.36 x2=19.78 y1=5.64 y2=4.22></line></svg></span></button><ul class="navbar-nav fork-me order-3 order-md-5"><li class=nav-item><a class=nav-link href=https://github.com/open-joinbase/JoinBase><svg class="feather feather-github" viewbox="0 0 24 24" fill=none height=20 stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 width=20 xmlns=http://www.w3.org/2000/svg><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg><span class="ms-2 visually-hidden">GitHub</span></a><li class=nav-item><a class=nav-link href=https://twitter.com/joinbase_db><svg class="feather feather-twitter" viewbox="0 0 24 24" fill=none height=20 stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 width=20 xmlns=http://www.w3.org/2000/svg><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg><span class="ms-2 visually-hidden">Twitter</span></a></ul><div class="collapse navbar-collapse order-4 order-md-1"><ul class="navbar-nav main-nav me-auto order-5 order-md-2"><li class=nav-item><a class=nav-link href=https://joinbase.io/docs/getting-started/introduction/>Docs</a><li class=nav-item><a class=nav-link href=https://joinbase.io/products/>Products</a><li class="nav-item blog active"><a class=nav-link href=https://joinbase.io/blog/>Blog</a><li class=nav-item><a class=nav-link href=https://joinbase.io/benchmark/>Benchmark</a><li class=nav-item><a class=nav-link href=https://joinbase.io/ranking/>Ranking</a><li class=nav-item><a class=nav-link href=https://joinbase.io/community/>Communities</a></ul><div class="break order-6 d-md-none"></div><form class="navbar-form flex-grow-1 order-7 order-md-3"><input aria-label="Search docs..." class="form-control is-search" placeholder="Search docs..." autocomplete=off id=userinput type=search><div class="shadow bg-white rounded" id=suggestions></div></form></div></div></header><div class="wrap container" role=document><div class=content><div class="row justify-content-center"><div class="col-md-12 col-lg-12 col-xl-12"><article><div class=blog-header><h1>TinyAnimal: Animal Recognition Practices on Grove Vision AI</h1><p><small>Posted February 20, 2023 by <a class="stretched-link position-relative" href=https://joinbase.io/contact/joinbase0/>joinbase0</a> ‐ <strong>9 min read</strong></small><p></div><p class=lead>In the TinyAnimal project, I will present the practices of EdgeML on the real-world cheap edge AI hardware.<div class=text-center><img alt=tinyanimal class=img-fluid src=/imgs/blog/tinyanimal/tinyanimal.jpg><p align=center>The wild without a reliable internet connection<p></div><h1 id=story>Story</h1><h3 id=problem>Problem</h3><p>There are many projects focus on the hardware of edge AI/ML. However, in real scenarios, there is no significant practice of learning details in software side on top of products to show, and this paper makes up for this deficiency.<p>At the same time, <strong>this project provides a complete and reproducible work flow of EdgeML/TinyML for animal recognition on one cheap edge AI hardware</strong>, which is rare in existing projects as known.<h3 id=hardware>Hardware</h3><div class=text-center><img alt=hardware class=img-fluid src=/imgs/blog/tinyanimal/hardware.jpg><p align=center>Wio terminal and Grove AI Module in SenseCAP K1100 kit<p></div><p>The project's hardware is the Grove Vision AI Module in Seeed <a href=https://www.seeedstudio.com/Seeed-Studio-LoRaWAN-Dev-Kit-p-5370.html>SenseCAP K1100/A1100</a>. There is standalone version of <a href=https://wiki.seeedstudio.com/Grove-Vision-AI-Module/>Grove Vision AI Module</a> in the official store.<p>The Vision AI Module has a chip: Himax HX6537-A. The mcu on the chip is based on the <a href=https://en.wikipedia.org/wiki/ARC_(processor)>ARC</a> arch which is unfamiliar to consumers. The main frequence is 400Mhz which is also not high. But the most interesting that the HX6537-A, has fast <a href=https://www.synopsys.com/designware-ip/technical-bulletin/performance-coding-advantages.html>XY SDRAM memory architecture to accelerate</a> the TinyML, like tensorflow lite model inferenece. We will see the performance of this chip later.<h3 id=workflow>Workflow</h3><div class=text-center><img alt=workflow class=img-fluid src=/imgs/blog/tinyanimal/workflow.jpg><p align=center>Workflow for TinyAnimal<p></div><p>The above workflow is common and clear. We only discuss some interesting requirements:<ol><li>The dataset is the public dataset with 9.6GB images.</ol><p>This avoids the common problem of too few samples or insufficient representativeness.<ol start=2><li>The training is completed locally.</ol><p>This avoids the common problem of too few samples or insufficient representativeness.<ol start=3><li>The data collection and realtime analysis is done via an edge database <a href=https://joinbase.io/>JoinBase</a>.</ol><p>Unlike the common databases like PostgreSQL or TimescaleDB, the JoinBase accept the MQTT message directly. Unlike the cloud service, the JoinBase support run in the edge which can be used in an environment without a network. Finally, the JoinBase is free for the commerical use which is also nice for further development of the edge platform.<h3 id=prepare-dataset>Prepare Dataset</h3><div class=text-center><img alt=dataset class=img-fluid src=/imgs/blog/tinyanimal/dataset.jpg><p align=center>Overall of "animials-80" dataset<p></div><p>At present, there is not much public research on the workedge AI for wildlifes or animals. One of the few publicly available animal datasets - <a href=https://www.kaggle.com/code/majdikarim/farm-animals-detection-yolov5>Animals Detection Images Dataset</a> from Kaggle (called "animials-80" dataset ) has been used. It contains 80 animals in 9.6GB images, and should be great enough for common animal recognition task.<h3 id=prepare-training-data>Prepare Training Data</h3><p>The good thing of animals-80 dataset is that it has been labeled itself. But the original label format is not Yolov5 label format. A preparation work has been carried on it. The core part is the preprocessing function shown above. Please the later code repo for more.<h3 id=train>Train</h3><p>Because we don't have enough resources to do a full training on full 9.6GB training. So, a picked subset of <strong>animials-80</strong> dataset has been choosen.<ol><li><strong>15-animal Subset Training</strong></ol><div class=text-center><img alt=15train class=img-fluid src=/imgs/blog/tinyanimal/15train.jpg><p align=center>15-animal subset training<p></div><p>We use a 24c/48T Xeon Platinum 8260 Processor to do the training using above the commands got from offical example.<pre class=language-tom data-lang=tom style=background-color:#2b303b;color:#c0c5ce;><code class=language-tom data-lang=tom><span>python3 train.py --img 192 --batch 32 --epochs 200 --data data/animal.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name animals --cache --project runs/train2
</span></code></pre><p>However, after two hours (Yes, it proves again that <strong>Don't use CPU to train</strong> even it is a top Xeon SP), the final recognition effect is found to be very poor.<blockquote><p>The main metrics are very low: the precision is 0.6, the recall and mAP_0.5 are just around 0.3.</blockquote><p>In fact, this result is close to not working.<ol start=2><li><strong>4-animal Subset Training</strong></ol><p>Let's reduce the types of recognized animals to four: spider, duck, magpie and butterfly, which of course are the most common animals in a suburban wild area.<p>Note, to re-run the preparing script to generate correct data/animal.yaml.<div class=text-center><img alt=4train class=img-fluid src=/imgs/blog/tinyanimal/4train.jpg><p align=center>4-animal subset training<p></div><blockquote><p>The main metrics become better: the precision is ~0.81, the recall and mAP_0.5 are around 0.6.</blockquote><p>We will review the performance of this model in the late inference trials and evalutions. It is possible to do only a binary categorization: one animal and no animal. But in this project, I look forward to evaluating the recognition effect in more complex scenes.<ol start=3><li><strong>4-animal Subset Training by YoLov5Official Pretrain Model</strong></ol><p>The above trainings are done by the <a href=https://wiki.seeedstudio.com/Train-Deploy-AI-Model-A1101-Grove-Vision-AI/>Seeed's official document's recommend</a>. The pretrained model is yolov5n6-xiao which may lack good generalization ability. In this project, we try a YoLOv5 official smallest pretrained model yolov5n6 to see whether there is some difference.<div class=text-center><img alt=yolov5n6 class=img-fluid src=/imgs/blog/tinyanimal/yolov5n6.jpg><p align=center>4-animal subset training with offical yolov5n6 model<p></div><p>The above result is obtained from the official yolov5n6 model with epochs=150. The result is great. Because,<blockquote><p>The main metrics: the precision, the recall and mAP_0, 5 are all larger than 0.9. In the ML, the difference in mAP_0.5 between 0.6 and 0.9 is huge and huge in real-world detection.</blockquote><p>Unfortunately, the final model trained based on the official yolov5n6 is close to 4MB, while the Grove AI module has the constraint model size no more than 1MB. So, we can not make use of any such bigger models (tried). <em><strong>Some suggestions will be discussed in the final section.Inference</strong></em><div class=text-center><img alt=3animals class=img-fluid src=/imgs/blog/tinyanimal/3animals.jpg><p align=center>Three examples of detections in the simulation<p></div><p>After the above traning, we do a picture based simulation to preliminarily evalute the effect of the model. Let's see examples.<p>The above is the output of Grove AI module. The index of classification is in the middle and the confidence is around the side. The animal name of coressponding index can be seen in above training figures.<p>The first and second detection are right in a nice confidence and the third detection is wrong. The third picture shows a magpie fly in the sky and the inference result is the butterfly. We just see the impact of this classification model in the later real-wprld evalution.<h3 id=real-world-evaluation>Real-world Evaluation</h3><p>Inferencing in real world is more challenging than inferencing in laboratory. Because the environment or the status of the tester or the tested object under the test can all have a big impact on the results. That is why we are planning in the workflow section.<p>We have done a real-world evaluation via a country park wildlife survey in the <a href=https://www.hackster.io/surfeit/tinywild-make-wild-iot-in-your-hand-729732>project TinyWild</a>. Two types of detections are carried out:<ol><li><strong>Dynamic Viewport (Moving Camera) Based Detection</strong></ol><div class=text-center><img alt=result class=img-fluid src=/imgs/blog/tinyanimal/result.png><p align=center>Classification statistics (with confidence > 75) in the whole survey<p></div><p>The above figure is the classification statistics (with confidence > 75) in the whole survey. There is a large time in which the camera is moving. So, this is a dynamic viewport (moving camera) based detection. "Unkown" and empty animal which stems from the software logics has been excluded here.<p>The basic conclusion is that, for individual identification, it is not particularly ideal, but the qualitative information collected is effective.<blockquote><p>Bufferfly is relatively outstanding in the statistics but without Magpie that've seen many times in the park.</blockquote><p>This seems the Magpie are been recongized as the bufferfly as shown in the analysis of the Inference section above. What they have in common is that, they often flys in the air. <strong>Three real-world factors: moving camera, moving objects and low resolution, have a great impact on the recognition results.</strong><ol start=2><li><strong>Static Viewport (Static Camera) Based Detection</strong></ol><p>To reduce impact of moving factors, a dedicated wild duck (mallard) observation in the lakeide has been carried out as well.<div class=text-center><img alt=duck_detection class=img-fluid src=/imgs/blog/tinyanimal/duck_detection.gif><p align=center>Primary process of the static wild duck observation<p></div><p>In the above first of captures, the count of duck in our frontend UI (one of interesting in this is that the dynamic table in UI is driven by a SQL query, please see our more infos in future projects) is 10. Suddenly, two ducks swims into the scope of camera. The count of duck has been increased to 13. Considering the orignal duck is counted, the 13 is the exacting count at that moment. It is found that <strong>the Grove AI works greatly for nearby animal detection</strong> like we done in lakeside: we got three counts when suddenly three ducks swims into the scope of camera in a relative static positioning. (note: in the TinyWild project, we said there are four counts, but it should be corrected to three counts according to our recordings.)<h3 id=ideas>Ideas</h3><p>Based on the above pratices, we give out the following suggestions for EdgeML or TinyML on a cheap edge AI hardware:<ol><li><strong>Try to observe statically</strong></ol><p>i.e. observers do not make large movements.<ol start=2><li><strong>Detect as few objects as possible</strong></ol><p>For example, only do the binary categorization: people or nobody, monkey or no monkey, bird or no bird.<ol start=3><li><strong>Make main metrics of model as large as possible</strong></ol><p>For example, the precision > 0.8, the recall and mAP_0.5 > 0.6.<ol start=4><li><strong>Improve recognition accuracy as possible (like, longer training time)</strong></ol><p>The cheap edge ML hardware usally has the limited resources, for example, Grove AI module has the constraint model size no more than 1MB, which falls below the model size trained from the yolov5's official yolov5n pre-trained network. The smaller model is found to significantly affect the model's primary metrics.<h3 id=code>Code</h3><p><a href=https://github.com/open-joinbase/yolov5-swift><strong>TinyAnimal 's modified yolov5-swift repo</strong></a><p><a href=https://github.com/open-joinbase/tinywild><strong>TinyAnimal 's modified Wio Terminal firmware and related</strong></a></article></div></div></div></div><footer class="site-footer text-muted"><div class=container><div class=row><div class="d-flex justify-content-between"><div class="col-sm-12 col-md-6"><h5>JoinBase</h5><p class=text-justify>A single-binary AIoT-first data-service platform.</div><div class="col-xs-6 col-md-2"></div><div class="col-xs-6 col-md-2"><h6>JoinBase</h6><ul class=footer-links><li><a href=#>Cloud</a><li><a href=/blog/>Blog</a><li><a href=/careers/>Careers</a><li><a href=/contact/>Contact</a><li><a href=/request/>Request</a></ul></div><div class="col-xs-6 col-md-2"><h6>Community</h6><ul class=footer-links><li><a href=https://github.com/open-joinbase/joinbase>GitHub</a><li><a href=https://www.youtube.com/channel/UCGCDg4je1pCTrtqU4dHiO1A>YouTube</a><li><a href=https://discord.gg/sqX6vfnURj>Discord</a><li><a href=https://twitter.com/joinbase_db>Twitter</a><li><a href=/community/>Communities</a></ul></div><div class="col-xs-6 col-md-2"><h6>More</h6><ul class=footer-links><li><a href=/docs/getting-started/>Get Started</a><li><a href=/docs/getting-started/introduction/>Documentation</a><li><a href=/benchmark/>Benchmark</a><li><a href=/docs/getting-started/tutorials/>Tutorials</a><li><a href=/community/>Community</a></ul></div></div></div></div><div class=container><div class="d-flex justify-content-between"><div><p class=copyright-text>Copyright © 2021 - 2022 <a href=#>JoinBase</a></div><div><p class=copyright-text><a href=/privacy-policy/>Privacy</a></div></div></div><script src="https://www.googletagmanager.com/gtag/js?id=G-6P71GFWL9M" async></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6P71GFWL9M');</script></footer><script defer src=https://joinbase.io/js/main.js></script><script defer src=https://joinbase.io/plugins/elasticlunr.min.js></script><script defer src=https://joinbase.io/search_index.en.js></script><script defer src=https://joinbase.io/js/search.js></script>